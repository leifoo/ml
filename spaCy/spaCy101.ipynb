{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy 101\n",
    "\n",
    "https://spacy.io/api/matcher\n",
    "Matcher · spaCy API DocumentationMatch sequences of tokens, based on pattern rulesspacy.io​\n",
    "    \n",
    "https://spacy.io/usage/rule-based-matching\n",
    "Rule-based matching · spaCy Usage DocumentationFind phrases and tokens, and match entitiesspacy.io​\n",
    "    \n",
    "http://markneumann.xyz/blog/dependency_matcher/![image.png](attachment:image.png)\n",
    "\n",
    "## Resources\n",
    "\n",
    "SOWA project\n",
    "previous name: IDS\n",
    "new name: PET EH\n",
    "SOWA (smart offsite Wells analysis) project\n",
    "webapp + dataiku + powerBI\n",
    "\n",
    "Water influx\n",
    "Webpage for labelling\n",
    "https://event-detector.azurewebsites.net/\n",
    " \n",
    "https://arxiv.org/pdf/1809.01478.pdf \n",
    "这个是ml model的文章\n",
    "然后rule-based event detection\n",
    "https://spacy.io/usage/rule-based-matching \n",
    "你有空看下这个\n",
    "这个比较重要 对于了解代码\n",
    "http://markneumann.xyz/blog/dependency_matcher/ \n",
    "我感觉最后这两个链接可以先看，对rule-based的了解比较有好处\n",
    "spacy是一个比较好的nlp框架/pipeline 我觉得你们今后改进也可以继续这个框架 因为各种NLP新东西一般都会跟Spacy对接\n",
    "\n",
    "我感觉如果从实践代码的角度 看spaCy doc比较好\n",
    "然后偏算法看 https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z \n",
    "这里会讲到各种经典算法 论文啥的!\n",
    "\n",
    "## Course\n",
    "\n",
    "https://spacy.io/usage/spacy-101\n",
    "\n",
    "Course\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/en/)\n",
    "\n",
    "## What's spaCy?\n",
    "\n",
    "spaCy is **a free, open-source library** for advanced **Natural Language Processing** (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for **production use** and helps you build applications that process and \"understand\" large volumes of text. It can be used to build **information extraction** or **natural language understanding** systems, or to pre-process text for **deep learning**.\n",
    "\n",
    "## Features\n",
    "\n",
    "| NAME | DESCRIPTION | \n",
    "| --- | --- | \n",
    "| Tokenization | Segmenting text into words, punctuations marks etc. | \n",
    "| Part-of-speech (POS) | Tagging Assigning word types to tokens, like verb or noun. |\n",
    "| Dependency Parsing | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object. |\n",
    "| Lemmatization\t| Assigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\". | \n",
    "| Sentence Boundary Detection (SBD) | Finding and segmenting individual sentences. | \n",
    "| Named Entity Recognition (NER) | Labelling named \"real-world\" objects, like persons, companies or locations. | \n",
    "| Entity Linking (EL) | Disambiguating textual entities to unique identifiers in a Knowledge Base. |\n",
    "| Similarity | Comparing words, text spans and documents and how similar they are to each other. | \n",
    "| Text Classification | Assigning categories or labels to a whole document, or parts of a document. |\n",
    "| Rule-based Matching | Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions. |\n",
    "| Training | Updating and improving a statistical model's predictions. |\n",
    "| Serialization | Saving objects to files or byte strings. |\n",
    "\n",
    "## Statistical models\n",
    "While some of spaCy's features work independently, others require [statistical models](https://spacy.io/models) to be loaded, which enable spaCy to predict linguistic annotations – for example, whether a word is a verb or a noun. spaCy currently offers statistical models for a variety of languages, which can be installed as individual Python modules. Models can differ in size, speed, memory usage, accuracy and the data they include. The model you choose always depends on your use case and the texts you’re working with. For a general-purpose use case, the small, default models are always a good start. They typically include the following components:\n",
    "\n",
    "- **Binary weights** for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.\n",
    "- **Lexical entries** in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n",
    "- **Data files** like lemmatization rules and lookup tables.\n",
    "- **Word vectors**, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n",
    "- **Configuration options**, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model.\n",
    "\n",
    "## Linguistic annotations\n",
    "\n",
    "spaCy provides a variety of linguistic annotations to give you **insights into a text's grammatical structure**. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context.\n",
    "\n",
    "LOADING MODELS\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "Once you’ve downloaded and installed a model, you can load it via spacy.load(). This will return a Language object containing all components and data needed to process text. We usually call it nlp. Calling the nlp object on a string of text will return a processed Doc:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "```\n",
    "\n",
    "Even though a Doc is processed - e.g. split into individual words and annotated - it still holds **all information of the original text**, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you'll never lose any information when processing text with spaCy.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "During processing, spaCy first **tokenizes** the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off - whereas \"U.K.\" should remain one token. Each Doc consists of individual tokens, and we can iterate over them:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. **Does the substring match a tokenizer exception rule?** For example, \"don't\" does not contain whitespace, but should be split into two tokens, “do” and \"n't\", while \"U.K.\" should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If there's a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split **complex**, **nested tokens** like combinations of abbreviations and multiple punctuation marks.\n",
    "\n",
    "While punctuation rules are usually pretty general, tokenizer exceptions strongly depend on the specifics of the individual language. This is why each [available language](https://spacy.io/usage/models#languages) has its own subclass like English or German, that loads in lists of hard-coded data and exception rules.\n",
    "\n",
    "### Part-of-speech tags and dependencies\n",
    "\n",
    "After tokenization, spaCy can **parse** and **tag** a given `Doc`. This is where the statistical model comes in, which enables spaCy to **make a prediction** of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
    "\n",
    "Linguistic annotations are available as [Token attributes](https://spacy.io/api/token#attributes). Like many NLP libraries, spaCy **encodes all strings to hash values** to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore `_` to its name:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "```\n",
    "\n",
    "Using spaCy's built-in [displaCy visualizer](https://spacy.io/usage/visualizers)\n",
    "\n",
    "### Named Entities\n",
    "\n",
    "A named entity is a \"real-world object\" that's assigned a name - for example, a person, a country, a product or a book title. spaCy can **recognize** [various types](https://spacy.io/api/annotation#named-entities) of named entities in a document, by asking the model for a **prediction**. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the `ents` property of a `Doc`:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "```\n",
    "\n",
    "### Word vectors and similarity \n",
    "\n",
    "Similarity is determined by comparing **word vectors** or \"word embeddings\", multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like [word2vec](https://en.wikipedia.org/wiki/Word2vec) and usually look like this:\n",
    "\n",
    "Models that come with built-in word vectors make them available as the [Token.vector](https://spacy.io/api/token#vector) attribute. [Doc.vector](https://spacy.io/api/doc#vector) and [Span.vector](https://spacy.io/api/span#vector) will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "```    \n",
    "\n",
    "If your application will benefit from a **large vocabulary** with more vectors, you should consider using one of the larger models or loading in a full vector package, for example, en_vectors_web_lg, which includes over **1 million unique vectors**.\n",
    "\n",
    "spaCy is able to compare two objects, and make a prediction of **how similar they are**. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one.\n",
    "\n",
    "Each `Doc`, `Span` and `Token` comes with a [`.similarity()`](https://spacy.io/api/token#similarity) method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective - whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. spaCy's similarity model usually assumes a pretty general-purpose definition of similarity.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "```\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "When you call `nlp` on a text, spaCy first tokenizes the text to produce a `Doc` object. The `Doc` is then processed in several different steps - this is also referred to as the **processing pipeline**. The pipeline used by the [default models](https://spacy.io/models) consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "![this is caption](img/pipeline.svg)\n",
    "\n",
    "| NAME | COMPONENT | CREATES | DESCRIPTION | \n",
    "| --- | --- | --- | --- |\n",
    "| tokenizer | [Tokenizer](https://spacy.io/api/tokenizer) | `Doc`\t| Segment text into tokens. |\n",
    "| tagger | [Tagger](https://spacy.io/api/tagger) | `Doc[i].tag` | Assign part-of-speech tags. | \n",
    "| parser | [DependencyParser](https://spacy.io/api/dependencyparser) | `Doc[i].head`, `Doc[i].dep`, `Doc.sents`, `Doc.noun_chunks` | Assign dependency labels. |\n",
    "| ner | [EntityRecognizer](https://spacy.io/api/entityrecognizer) | `Doc.ents`, `Doc[i].ent_iob`, `Doc[i].ent_type` | Detect and label named entities. |\n",
    "| textcat | [TextCategorizer](https://spacy.io/api/textcategorizer) | `Doc.cats` | Assign document labels. |\n",
    "| … | [custom components](https://spacy.io/usage/processing-pipelines#custom-components) | `Doc._.xxx`, `Token._.xxx`, `Span._.xxx` | Assign custom attributes, methods or properties. | \n",
    "\n",
    "The processing pipeline always **depends on the statistical model** and its capabilities. For example, a pipeline can only include an entity recognizer component if the model includes data to make predictions of entity labels. This is why each model will specify the pipeline to use in its meta data, as a simple list containing the component names:\n",
    "\n",
    "```python\n",
    "\"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
    "```\n",
    "\n",
    "## Vocab, hashes and lexemes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
