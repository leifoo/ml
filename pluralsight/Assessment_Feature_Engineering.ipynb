{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\n",
    "## First Assessment\n",
    "\n",
    "1. You are testing the null hypothesis for several groups in a given dataset using analysis of variance (ANOVA) and your calculation of the F-statistic is 12. What information is missing in order to reject the null hypothesis or not?\n",
    "    - [ ] There is no missing information, any F > 0 indicates the null hypothesis should be rejected.\n",
    "    - [ ] The between and within sum of squares, and the selected critical value alpha.\n",
    "    - [ ] There is no missing information, any F > 1 indicates the null hypothesis should be rejected.\n",
    "    - [x] <span style='background:yellow'>The degrees of freedom of the between and within variabilities, and the selected critical value alpha.</span>\n",
    "\n",
    "\n",
    "2. You are implementing a nearest neighbor classifier on a dataset whose features need to be scaled. How can you use the MinMaxScaler method to achieve this task?\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform(features)\n",
    "```\n",
    "\n",
    "\n",
    "3. What is a text corpus?\n",
    "    - [ ] The body of a text (as opposed to the introduction and conclusion).\n",
    "    - [ ] A subset of documents resulting from a query in a system.\n",
    "    - [ ] The structure of a given text.\n",
    "    - [x] <span style='background:yellow'>The entire set of documents involved in a system.</span>\n",
    "\n",
    "\n",
    "4. The principal component in the principal component analysis (PCA) procedure has the largest what?\n",
    "    - [ ] Entropy\n",
    "    - [ ] Mean\n",
    "    - [x] Variance\n",
    "    - [ ] Significance\n",
    "\n",
    "\n",
    "5. Which of the following algorithms can be used to assess the following relationship: King is to Queen as Husband is to Wife?\n",
    "    - [ ] Bag-of-n-grams\n",
    "    - [x] <span style='background:yellow'>Word2Vec</span>\n",
    "    - [ ] Parts-of-speech\n",
    "    - [ ] Bag-of-Words\n",
    "\n",
    "\n",
    "6. What are stop words?\n",
    "    - [ ] The final word of every sentence.\n",
    "    - [ ] Very rare words in the text corpus.\n",
    "    - [ ] The word that separates two topics in the text.\n",
    "    - [x] <span style='background:yellow'>Very common words in the language (e.g. the, a, is).</span>\n",
    "\n",
    "\n",
    "7. What do Parts-of-speech (POS) tags identify?\n",
    "    - [x] <span style='background:yellow'>The role a word has in a particular sentence (i.e. the noun, article, conjunction, etc.).</span>\n",
    "    - [ ] The role a word performs in a sentence (i.e. obscure can be a noun and a verb).\n",
    "    - [ ] The role a sentence has in a text (i.e. topic, support, transition).\n",
    "    - [ ] The logical propositions given by the text (i.e. P or Q > R ).\n",
    "\n",
    "\n",
    "8. You have an array of given values: `['#$%', 'ALpd', '123', 89]` How can you label encode this data using Scikit-Learn method?\n",
    "    - `sklearn.preprocessing.LabelEncoder().fit_transform(['#$%', 'ALpd', '123', 89])`\n",
    "\n",
    "\n",
    "9. You have a coordinate vector of: [4, 5] What is the L1 norm of this vector?\n",
    "    - 9\n",
    "\n",
    "\n",
    "10. You are using an autoencoder for input x. After a forward pass to compute activations of all the hidden layers and obtain an output x', what is the next step?\n",
    "    - [ ] Use a softmax layer to perform feature selection.\n",
    "    - [ ] Use max-pooling to reduce dimensionality.\n",
    "    - [ ] Compute the sigmoid of x' and subtract x from it.\n",
    "    - [x] Measure the error that deviates the output x' from the input x.\n",
    "\n",
    "\n",
    "11. When using a bag-of-words (BoW) representation of a document for sentiment analysis, many of the predictions fail to take into account the effect of negations. This results in misclassifications such as, \"I'm not very happy about this\" being assigned a positive sentiment. What is the most likely explanation for this?\n",
    "    - [ ] The dataset is clearly mislabeled; BoW representations should work in this scenario.\n",
    "    - [ ] There are too few samples that include negations in the dataset.\n",
    "    - [x] BoW representations cannot model the role of the word in sentences.\n",
    "    - [ ] BoW representations cannot be used effectively for tasks involving classification.\n",
    "\n",
    "\n",
    "12. Given the ngrams(text, n) function below, which statement is the correct option to generate n-grams from a given text?\n",
    "```python\n",
    "def ngrams(text, n):\n",
    "    words = text.split(' ')\n",
    "    output = []\n",
    "    output_len = len(words)-n+1\n",
    "    for i in range(output_len):\n",
    "        # select the correct line for this line\n",
    "        output.append(' '.join(words[i:i+n]))\n",
    "    return output\n",
    "```\n",
    "\n",
    "\n",
    "13. What is model stacking?\n",
    "    - [ ] To use several models in parallel and average the outputs as the result.\n",
    "    - [ ] A training optimization technique that reuses the weights of previously trained models for similar architectures.\n",
    "    - [x] To use the output of a model as the input of another.\n",
    "    - [ ] To use several models in parallel and use the maximum output as the result.\n",
    "\n",
    "**Model stacking** is an efficient ensemble method in which the predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions.\n",
    "\n",
    "\n",
    "14. You have a dataset with unbalanced categorical data, leading to low accuracy. What can you do to improve the accuracy of the model?\n",
    "    - [ ] Redistribute observations between the training and test data to remove unidentified bias.\n",
    "    - [ ] Normalize numerical values to fall between -1 and 1, to prevent features with high values from being dominant.\n",
    "    - [x] <span style='background:yellow'>Resample the data to balance out the categories.</span>\n",
    "    - [ ] Distribute observations randomly across the dataset.\n",
    "\n",
    "\n",
    "15. You calculate between group variability V_b and within group variability V_w. What is the next step in a classical analysis of variance (ANOVA) test?\n",
    "    - [ ] Compute F = V_b - V_w\n",
    "    - [ ] Compute F = V_b * V_w\n",
    "    - [ ] Compute F = V_b + V_w\n",
    "    - [x] Compute F = V_b/V_w\n",
    "\n",
    "\n",
    "16. Which of the following statements correctly explains the rationale behind feature selection using a variance threshold?\n",
    "    - [ ] Features with values close to zero carry little information.\n",
    "    - [ ] Features with a mean value close to zero carry little information.\n",
    "    - [ ] Features spanning a short range carry little information.\n",
    "    - [x] <span style='background:yellow'>Features with little changes in the data carry little information.</span>\n",
    "\n",
    "\n",
    "17. In a bin counting scheme, what happens if data from a feature doesn't falls into any of the existing bins?\n",
    "    - [ ] The data is assigned to the bin with the highest number of values.\n",
    "    - [x] The data is assigned to a garbage bin.\n",
    "    - [ ] The data is assigned to the bin with the lowest number of values.\n",
    "    - [ ] The data is mapped to the last bin.\n",
    "\n",
    "\n",
    "18. Consider the task of counting the number of livestock in aerial images of farmlands. If the machine learning model that detects animals is only capable of dealing with images where the animal is in the center, what is a very important step in the pre-processing stage of the task?\n",
    "    - [ ] Transform the image so that all animals are centered and in different channels before sending it to the model.\n",
    "    - [x] <span style='background:yellow'>Assigning bounding boxes to all objects so that the object is centered in its box. Individual boxes should be used as input to the model.</span>\n",
    "    - [ ] Cut the image in squares of equal size. The input of the model should be the individual squares.\n",
    "    - [ ] Removing the background of the image, keeping only the pixels with animals in it.\n",
    "    \n",
    "\n",
    "## Second Assessment\n",
    "\n",
    "1. To use an image as input of a neural network, some preprocessing is required in order to make the format compatible with the network. Most architectures require no additional feature extraction. Which of the following statements explains the rationale behind this?\n",
    "    - [ ] The first layers of the neural network learn to extract the features it needs to perform its task.\n",
    "    - [ ] The non-linear nature of neural networks works better with unperturbed input. The backpropagation process is disrupted by transformations on the input images.\n",
    "    - [ ] Neural networks adapt themselves to any input, no matter what they are. This known as reinforcement learning, the neural network corrects itself, reinforcing its ability to deal with the input.\n",
    "    - [ ] Neural networks prefer to work with raw data. This allows the method to have more information in every step of the training.\n",
    "\n",
    "\n",
    "2. What is the formula for normalizing a data set to unit length?\n",
    "    - $x'=x/||x||$\n",
    "\n",
    "\n",
    "3. You have an array with both positive and negative values: $[-2, 5, 8, 10, -12]$. What is the value of element -2 when the data is normalized using min-max scaler?\n",
    "    - 0.45\n",
    "\n",
    "\n",
    "4. You are using k-PCA to classify a dataset with non-linearly separable data points. The dataset is so large that the kernel matrix does not fit into memory. What would you do to make the problem manageable?\n",
    "    - [ ] Use a Gaussian kernel to reflect local structure as opposed to global structure.\n",
    "    - [x] <span style='background:yellow'>Perform clustering on the dataset and use the means of the clusters as data points for the kernel matrix.</span>\n",
    "    - [ ] Project all the data points to a higher dimension before applying kernel PCA.\n",
    "    - [ ] Perform classical PCA on the data and reduce the dimensionality before using kernel PCA\n",
    "\n",
    "\n",
    "5. How would you automate Feature Engineering?\n",
    "    - [ ] Downloading, merging and filtering data can be automated with common tools. All other tasks have to be performed manually.\n",
    "    - [ ] Feature Engineering requires a deep understanding of the domain and currently cannot be automated.\n",
    "    - [x] <span style='background:yellow'>Many tasks like merging, feature extraction, building a feature matrix can be performed using available tools. This allows the automatic creation of features for new data.</span>\n",
    "    - [ ] Feature Engineering is done only once during model building. There is no need to automate it.\n",
    "\n",
    "\n",
    "6. Which task is usually associated with Feature Engineering?\n",
    "    - [x] <span style='background:yellow'>Imputing data</span>\n",
    "    - [ ] Plotting graphs\n",
    "    - [ ] Downloading data\n",
    "    - [ ] Formatting numerical data\n",
    "\n",
    "In statistics, **imputation** is the process of replacing missing data with substituted values\n",
    "\n",
    "\n",
    "7. You have a square 3x3 matrix:\n",
    "```python\n",
    "[\n",
    "[ -3., 5., 15 ], \n",
    "[ 0., 6., 14 ], \n",
    "[ 6., 3., 11 ]\n",
    "]\n",
    "```\n",
    "How can you perform quantization on the matrix resulting into an array of {3, 2, 2}bins?\n",
    "```python\n",
    "sklearn.preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2]).fit_transform(matrix)```\n",
    "\n",
    "8. What is another name for the test dataset?\n",
    "    - [ ] Verification dataset\n",
    "    - [x] Holdout dataset\n",
    "    - [ ] Unbiased dataset\n",
    "    - [ ] Cross-examination dataset\n",
    "\n",
    "\n",
    "9. Which of the following is a manifold learning method?\n",
    "    - [ ] Independent Component Analysis\n",
    "    - [ ] Principal Component Analysis\n",
    "    - [ ] Linear Discriminant Analysis\n",
    "    - [x] <span style='background:yellow'>t-Distributed Stochastic Neighbor Embedding</span>\n",
    "\n",
    "\n",
    "10. How does feature scaling improve the Gradient Descent process for a Linear Regression?\n",
    "    - [x] The descent steps are more direct towards the minimum and it also minimizes the risk of \"overshooting\" the minimum.\n",
    "    - [ ] Scaling down all input values reduces the size and number of steps needed to reach the minimum.\n",
    "    - [ ] Feature scaling does not effect the Gradient Decent process or outcome.\n",
    "    - [ ] The steps are generally on a larger scale and thus computationally more efficient.\n",
    "\n",
    "\n",
    "11. What does the chi-squared test evaluate?\n",
    "    - [x] <span style='background:yellow'>The likelihood that any observed difference between classes can occur at random. </span>\n",
    "    - [ ] The likelihood that the data can predict the labels.\n",
    "    - [ ] Whether the data points across classes share a probability density function.\n",
    "    - [ ] Whether the p-value of the observations has statistical significance.\n",
    "\n",
    "\n",
    "12. What is the difference between applying one-hot encoding and dummy encoding on an attribute with N classes?\n",
    "    - [ ] <span style='background:yellow'>One-hot encoding converts the attribute into N new attributes, whereas dummy encoding converts it into N-1 new attributes.</span>\n",
    "\n",
    "\n",
    "13. You are producing the word feature set for a standard information retrieval application. You split the text into words and remove capitalization for all common names. What is the next step?\n",
    "    - [x] <span style='background:yellow'>Filter stop words</span>\n",
    "    - [ ] Tokenization\n",
    "    - [ ] Vectorization\n",
    "    - [ ] Produce the one-hot encoding\n",
    "\n",
    "\n",
    "14. You are working in a limited resource environment where you need to encode a categorical feature for further processing. What type of encoder is suitable for this task?\n",
    "    - [ ] A label encoder, because it will map only existing values of the feature to corresponding continuous values.\n",
    "    - [x] <span style='background:yellow'>A label encoder, because it will map only existing values of the feature to corresponding discrete values.</span>\n",
    "    - [ ] A one-hot encoder, because it will create a new feature for every encountered unique class.\n",
    "    - [ ] A one-hot encoder, because it will diminish the feature size by keeping only one class out of all the correlated classes.\n",
    "\n",
    "\n",
    "15. What is the correct strategy to use when you have missing data in the target column?\n",
    "    - [ ] Replace the missing values with the Mean\n",
    "    - [ ] Replace the missing values using Principal Component Analysis\n",
    "    - [ ] Replace the missing values using Multivariate Imputation using Chained Equations (MICE)\n",
    "    - [x] Remove the entire row\n",
    "\n",
    "\n",
    "16. For which type of data is Principal Component Analysis (PCA) NOT the best option?\n",
    "    - [ ] When the variance of the data does not follow a normal distribution.\n",
    "    - [ ] When the variance of the data lies in a non-linear manifold.\n",
    "    - [ ] When the variance of the data is not proportional to the standard distribution.\n",
    "    - [x] When the variance of the data is better explained by non-orthogonal components.\n",
    "\n",
    "\n",
    "17. You have a dataset of flight delays and hourly weather readings which include the following data fields: `AirportCode`, `FlightNumber`, `Airline`, `Date`, `FlightDuration`, `DepartureDelayMinutes`, and `ArrivalDelayMinutes`. What steps would you follow to predict whether or not a given flight will be delayed?\n",
    "    1. Use feature engineering to convert the `ArrivalDelayMinutes` data field to a boolean value.\n",
    "    2. Train a Binary Classification model using this boolean value as the target.\n",
    "\n",
    "\n",
    "18. You are building a tool to detect plagiarism for English essays at a national level. What would you do to shrink the search space automatically?\n",
    "    - [ ] Use local alignment techniques to match documents.\n",
    "    - [x] <span style='background:yellow'>Use LSH to group documents which are likely to show high similarity.</span>\n",
    "    - [ ] Use PCA to reduce the dimensionality of the dataset.\n",
    "    - [ ] Compute a pairwise Jaccard Similarity of all documents.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
