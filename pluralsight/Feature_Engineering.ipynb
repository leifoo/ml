{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\n",
    "## First Try\n",
    "\n",
    "1. You are testing the null hypothesis for several groups in a given dataset using analysis of variance (ANOVA) and your calculation of the F-statistic is 12. What information is missing in order to reject the null hypothesis or not?\n",
    "    - [ ] There is no missing information, any F > 0 indicates the null hypothesis should be rejected.\n",
    "    - [ ] The between and within sum of squares, and the selected critical value alpha.\n",
    "    - [ ] There is no missing information, any F > 1 indicates the null hypothesis should be rejected.\n",
    "    - [x] <span style='background:yellow'>The degrees of freedom of the between and within variabilities, and the selected critical value alpha.</span>\n",
    "\n",
    "\n",
    "2. You are implementing a nearest neighbor classifier on a dataset whose features need to be scaled. How can you use the MinMaxScaler method to achieve this task?\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform(features)\n",
    "```\n",
    "\n",
    "\n",
    "3. What is a text corpus?\n",
    "    - [ ] The body of a text (as opposed to the introduction and conclusion).\n",
    "    - [ ] A subset of documents resulting from a query in a system.\n",
    "    - [ ] The structure of a given text.\n",
    "    - [x] <span style='background:yellow'>The entire set of documents involved in a system.</span>\n",
    "\n",
    "\n",
    "4. The principal component in the principal component analysis (PCA) procedure has the largest what?\n",
    "    - [ ] Entropy\n",
    "    - [ ] Mean\n",
    "    - [x] Variance\n",
    "    - [ ] Significance\n",
    "\n",
    "\n",
    "5. Which of the following algorithms can be used to assess the following relationship: King is to Queen as Husband is to Wife?\n",
    "    - [ ] Bag-of-n-grams\n",
    "    - [x] <span style='background:yellow'>Word2Vec</span>\n",
    "    - [ ] Parts-of-speech\n",
    "    - [ ] Bag-of-Words\n",
    "\n",
    "\n",
    "6. What are stop words?\n",
    "    - [ ] The final word of every sentence.\n",
    "    - [ ] Very rare words in the text corpus.\n",
    "    - [ ] The word that separates two topics in the text.\n",
    "    - [x] <span style='background:yellow'>Very common words in the language (e.g. the, a, is).</span>\n",
    "\n",
    "\n",
    "7. What do Parts-of-speech (POS) tags identify?\n",
    "    - [x] <span style='background:yellow'>The role a word has in a particular sentence (i.e. the noun, article, conjunction, etc.).</span>\n",
    "    - [ ] The role a word performs in a sentence (i.e. obscure can be a noun and a verb).\n",
    "    - [ ] The role a sentence has in a text (i.e. topic, support, transition).\n",
    "    - [ ] The logical propositions given by the text (i.e. P or Q > R ).\n",
    "\n",
    "\n",
    "8. You have an array of given values: `['#$%', 'ALpd', '123', 89]` How can you label encode this data using Scikit-Learn method?\n",
    "    - `sklearn.preprocessing.LabelEncoder().fit_transform(['#$%', 'ALpd', '123', 89])`\n",
    "\n",
    "\n",
    "9. You have a coordinate vector of: [4, 5] What is the L1 norm of this vector?\n",
    "    - 9\n",
    "\n",
    "\n",
    "10. You are using an autoencoder for input x. After a forward pass to compute activations of all the hidden layers and obtain an output x', what is the next step?\n",
    "    - [ ] Use a softmax layer to perform feature selection.\n",
    "    - [ ] Use max-pooling to reduce dimensionality.\n",
    "    - [ ] Compute the sigmoid of x' and subtract x from it.\n",
    "    - [x] Measure the error that deviates the output x' from the input x.\n",
    "\n",
    "\n",
    "11. When using a bag-of-words (BoW) representation of a document for sentiment analysis, many of the predictions fail to take into account the effect of negations. This results in misclassifications such as, \"I'm not very happy about this\" being assigned a positive sentiment. What is the most likely explanation for this?\n",
    "    - [ ] The dataset is clearly mislabeled; BoW representations should work in this scenario.\n",
    "    - [ ] There are too few samples that include negations in the dataset.\n",
    "    - [x] BoW representations cannot model the role of the word in sentences.\n",
    "    - [ ] BoW representations cannot be used effectively for tasks involving classification.\n",
    "\n",
    "\n",
    "12. Given the ngrams(text, n) function below, which statement is the correct option to generate n-grams from a given text?\n",
    "```python\n",
    "def ngrams(text, n):\n",
    "    words = text.split(' ')\n",
    "    output = []\n",
    "    output_len = len(words)-n+1\n",
    "    for i in range(output_len):\n",
    "        # select the correct line for this line\n",
    "        output.append(' '.join(words[i:i+n]))\n",
    "    return output\n",
    "```\n",
    "\n",
    "\n",
    "13. What is model stacking?\n",
    "    - [ ] To use several models in parallel and average the outputs as the result.\n",
    "    - [ ] A training optimization technique that reuses the weights of previously trained models for similar architectures.\n",
    "    - [x] To use the output of a model as the input of another.\n",
    "    - [ ] To use several models in parallel and use the maximum output as the result.\n",
    "\n",
    "**Model stacking** is an efficient ensemble method in which the predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions.\n",
    "\n",
    "\n",
    "14. You have a dataset with unbalanced categorical data, leading to low accuracy. What can you do to improve the accuracy of the model?\n",
    "    - [ ] Redistribute observations between the training and test data to remove unidentified bias.\n",
    "    - [ ] Normalize numerical values to fall between -1 and 1, to prevent features with high values from being dominant.\n",
    "    - [x] <span style='background:yellow'>Resample the data to balance out the categories.</span>\n",
    "    - [ ] Distribute observations randomly across the dataset.\n",
    "\n",
    "\n",
    "15. You calculate between group variability V_b and within group variability V_w. What is the next step in a classical analysis of variance (ANOVA) test?\n",
    "    - [ ] Compute F = V_b - V_w\n",
    "    - [ ] Compute F = V_b * V_w\n",
    "    - [ ] Compute F = V_b + V_w\n",
    "    - [x] Compute F = V_b/V_w\n",
    "\n",
    "\n",
    "16. Which of the following statements correctly explains the rationale behind feature selection using a variance threshold?\n",
    "    - [ ] Features with values close to zero carry little information.\n",
    "    - [ ] Features with a mean value close to zero carry little information.\n",
    "    - [ ] Features spanning a short range carry little information.\n",
    "    - [x] <span style='background:yellow'>Features with little changes in the data carry little information.</span>\n",
    "\n",
    "\n",
    "17. In a bin counting scheme, what happens if data from a feature doesn't falls into any of the existing bins?\n",
    "    - [ ] The data is assigned to the bin with the highest number of values.\n",
    "    - [x] The data is assigned to a garbage bin.\n",
    "    - [ ] The data is assigned to the bin with the lowest number of values.\n",
    "    - [ ] The data is mapped to the last bin.\n",
    "\n",
    "\n",
    "18. Consider the task of counting the number of livestock in aerial images of farmlands. If the machine learning model that detects animals is only capable of dealing with images where the animal is in the center, what is a very important step in the pre-processing stage of the task?\n",
    "    - [ ] Transform the image so that all animals are centered and in different channels before sending it to the model.\n",
    "    - [x] <span style='background:yellow'>Assigning bounding boxes to all objects so that the object is centered in its box. Individual boxes should be used as input to the model.</span>\n",
    "    - [ ] Cut the image in squares of equal size. The input of the model should be the individual squares.\n",
    "    - [ ] Removing the background of the image, keeping only the pixels with animals in it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
