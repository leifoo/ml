{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Summer School\n",
    "\n",
    "http://mlss.tuebingen.mpg.de/2020/schedule.html\n",
    "\n",
    "## 1. Symbolic, Statistical and Causal Artificial Intelligence\n",
    "\n",
    "Lecturer: Bernhard Schölkopf\n",
    "\n",
    "Motivations:\n",
    "\n",
    "> What I cannot create, I do not understand. \n",
    "- Richard Feynman\n",
    "\n",
    "### Cybernetics - 1940s/50s\n",
    "\n",
    "- Norbert Weiner. _Cybernetics or Control and Communication in the Animal and the Machine_ (1948)\n",
    "- Study of control and information processing (rather than energy) in animals and machines\n",
    "- Macy Conferences 1946-53: \"Circular Causal and Feedback Mechanisms in Biological and Social Systems\". Birth of cybernetics and cognitive science.\n",
    "- John von Neumann, Alan Turing, Claude Shannon\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "Rosenblatt's (1958) perceptron schemes\n",
    "\n",
    "A particular task that could __not__ be learned (Hawkins, 1961)\n",
    "- 'and' can be done with one layer, 'xor' requires a cascade.\n",
    "\n",
    "Perception limitations recognized by Rosenblatt\n",
    "\n",
    "Percepton Convergence Theorem (Novikoff, 1962)\n",
    "\n",
    "Symbolic Al\n",
    "- Dartmouth Summer School, 1956\n",
    "- intelligence is a process of manipulating discrete symbols; John McCarthy, Allen Newell, Herb Simon, Marvin Minsky\n",
    "\n",
    "Newell & Simon:\n",
    "_The Physical Symbol system Hypothesis_. A physical symbol system has the necessary and sufficient means for general intelligent action.\n",
    "\n",
    "Minsky & Papert recall (1988/89)\n",
    "\n",
    "Hecht-Nielssen (1990)\n",
    "\n",
    "The \"XOR Affair\"\n",
    "- Minsky & Papert (1969): _Perceptrons_\n",
    "\n",
    "Technical content of Perceptions\n",
    "- assume 1 output neuron\n",
    "- restrict the \"order\" of the association units\n",
    "- parity cannot be solved unless the order equals the whole retina\n",
    "- similar for figure/ground (connectedness)\n",
    "- both can easily be solved using serial algorithms\n",
    "\n",
    "Is parity important?\n",
    "\n",
    "Reception of \"Perceptrons\"\n",
    "\n",
    "The end of perceptions (cf. Olazaran, 1996)\n",
    "- symbolic AI was gaining momentum\n",
    "- of the major group, only Rosenblatt continued, but dired in 1971\n",
    "- ARPA decided to back symbolic AI and cut off neural nets\n",
    "- The defeat of neural nets helped legitimitize the symbolic AI\n",
    "\n",
    "Predictions of symbolic AI\n",
    "- Herb Simon (1957) predicted\n",
    "- Symbolic AI gave birth to Computer Science\n",
    "\n",
    "The Return of Neural Nets\n",
    "- symbolic AI did poorly at speech and vision\n",
    "- computing became a commodity\n",
    "- the PDP group was formed by psychologiest\n",
    "- Boltzmann machine (Hinton, Sejnowski)\n",
    "\n",
    "The Return of Neural Nets, II\n",
    "- Back-propagation, mid-1980s (Rumelhart, Hinton, Williams, LeCun, Werbos, Amari, Linnainmaa, Kelley, Brison, ...)\n",
    "- Minsky & Papert (1988, Perceptrons, 2nd ed.)\n",
    "\n",
    "\n",
    "Machine Learning\n",
    "- _Laplace_. Introduced _Bayes_' Theorem / inverse probability in the gene applied it to celestial mechanics; _Gauss_ \n",
    "- _Solomonoff_ (1950s): probabilistic AI\n",
    "- Statistical Learning Theory: Vapnik & Chervonenkis (ca. 1968-1982) leading to PAC (1984) \n",
    "- physicists, e.g. John Hopfield (1982) (Ising model) \n",
    "- Probabilistic Expert Systems / knowledge representation (Pearl 1988); Bayes nets \n",
    "- first UAI (1985) \n",
    "- first NIPS (1987) \n",
    "- Probabilistic foundations (1990s) - MacKay, Neal, Jordan, Hinton, Bishop,... - SVMs & kernel methods (1990) - Vapnik et al.\n",
    "\n",
    "Classic AI (rules shaped by humans)\n",
    "- Applicable if a human provides a precise model of what the program should do\n",
    "    - AI conferences (AAAI + IJCAI) \n",
    "- Machine Learning (rules shaped by learning) Applicable also on complex tasks where nobody can specify a model\n",
    "    - ML conferences (NIPS + ICML + CVPR) \n",
    "    \n",
    "https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/\n",
    "\n",
    "Big Data\n",
    "Sonnenburg, Rätsch, Schäfer, 60 Schölkopf, 2006, Journal of 50 Machine Learning Research \n",
    "- Task: classify human DNA sequence 30 locations into {acceptor splice site, decoy} using 15 Million sequences of length 141, and a Multiple-Kernel Support Vector Machines. \n",
    "- PRC = Precision-RecaII-Curve, fraction of correct positive predictions among all 1000 positively predicted cases\n",
    "\n",
    "- High dimensional & complex regularities.\n",
    "- little mechanistic understanding (e.g., medical problems)\n",
    "- large training sets \n",
    "- __optimality in the limit__ - (independent identically distributed) i.i.d. data (no change between training and test)\n",
    "\n",
    "Human-level control through deep reinforcement learning (Mnihl) Nature\n",
    "\n",
    "News & Views - Learning to see and act, \n",
    "Multiple trends at work \n",
    "- High capacity machine learning systems \n",
    "- Massive amounts of data, by simulation or human labeling (Amazon Mechanical Turk, etc.) \n",
    "- GPUs for massively parallel computation \n",
    "- The __settings are i.i.d.__, or are made i.i.d.\n",
    "\n",
    "Human-level object recognition?\n",
    "- from Perona, 2017; cf. Lopez-Paz et al., 2016\n",
    "\n",
    "Machine learning uses correlations rather than causality\n",
    "\n",
    "Adversarial Vulnerability \n",
    "- Image credit: http://people.csail.mit.edu/madry/lab/blog/adversarial/2018/07/06/adversarial_intro/ \n",
    "- C. Szegedy et al. Intriguing properties of neural networks. arXiv:1312.6199, 2013\n",
    "\n",
    "The New England Journal of Medicine \n",
    "- Freedman, 2012, Association of Coffee Drinking with Total and Cause-Specific Mortality\n",
    "\n",
    "Dependence vs. Causation \n",
    "\n",
    "\"Correlation does not tell us anything about causality\"\n",
    "- Better to talk of dependence than correlation \n",
    "- Most statisticians would agree that causality does tell us something about dependence \n",
    "- But dependence does tell us something about causality too:\n",
    "\n",
    "Reichenbach's Common Cause Principle\n",
    "- (i) if X and Y are dependent, then there exists Z _causally_ influencing both;\n",
    "- (ii) Z screens X and Y from each other (given Z, X und Y become independent)\n",
    "\n",
    "\"cargo cult\"\n",
    "> \"imitate the superficial exterior of a process or system without having any understanding of the underlying substance\". (source: http://philosophyisfashionable.blogspot.com/) \n",
    "\n",
    "- for prediction in the IID setting, imitating the exterior of a process is enough (i.e., can disregard causal structure) \n",
    "- anything else can benefit from causal learning\n",
    "\n",
    "- 1./2. industrial revolution Generate/convert/process energy digital revolution \n",
    "- / \"big data\" / cybernetics: Generate/convert/process information\n",
    "    - 1st phase: by computers beginning computer science the first programming languages \n",
    "    - 2nd phase: add another source of information, non-structural and weak in distributed over large datasets, in order to process, you need machine learning\n",
    "\n",
    "It's our information processing abilities that make us human and that are the basis of what we are doing to this planet and how we are dominating these planets. We are not proud of being particularly good at energy processing, we are particularly strong we're good all around us, but we think we are somehow special in terms of our information processing abilities. So if this is now done by machines, it will have major consequences. \n",
    "\n",
    "<div style=\"width:400 px; font-size:100%; text-align:center;\"> <center><img src=\"img/book.png\" width=400px alt=\"fig_book\" style=\"padding-bottom:1.0em;padding-top:2.0em;\"></center>Figure 1. Books</div>\n",
    "\n",
    "Summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
